{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIgAR1swre21ezb9HoCtZS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kodeu/trading-lstm/blob/main/lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dPWWzvd5Xu4",
        "outputId": "7ef51b84-92da-48a0-d8b1-1053fad9ecdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X var\n",
            "[[[0.84735415 0.96474923 0.20622754 0.61348274 0.90241036]\n",
            "  [0.73208517 0.60125931 0.14470846 0.87736497 0.45576489]\n",
            "  [0.85187066 0.36235962 0.92125042 0.54080452 0.6431247 ]\n",
            "  [0.2133932  0.35341419 0.91143929 0.47364826 0.61131851]\n",
            "  [0.2741732  0.05934346 0.2691215  0.00617203 0.68420291]\n",
            "  [0.03180913 0.85683197 0.71924531 0.66176876 0.54616098]\n",
            "  [0.47328631 0.87679091 0.51247305 0.04484821 0.57804854]\n",
            "  [0.41508173 0.25075108 0.7482694  0.22121309 0.69592326]\n",
            "  [0.61668526 0.16758549 0.56635136 0.68931852 0.39528449]\n",
            "  [0.54163887 0.2737599  0.86423515 0.54560132 0.98092686]]\n",
            "\n",
            " [[0.62919728 0.78191398 0.69628464 0.13690142 0.35101299]\n",
            "  [0.72207753 0.30638813 0.96486645 0.12924175 0.46162097]\n",
            "  [0.12827089 0.63350125 0.86191557 0.08851617 0.77551968]\n",
            "  [0.59927513 0.99056334 0.85837896 0.52565848 0.60860976]\n",
            "  [0.9245842  0.41812766 0.69358121 0.73650082 0.82864169]\n",
            "  [0.26824393 0.79509208 0.94158631 0.92520852 0.9402791 ]\n",
            "  [0.19604043 0.04553375 0.84830167 0.03181968 0.10177451]\n",
            "  [0.38486472 0.12752367 0.19152228 0.15344137 0.16639731]\n",
            "  [0.74391507 0.09331977 0.76242544 0.00678404 0.92711466]\n",
            "  [0.54969187 0.31325613 0.98716365 0.03004832 0.22440846]]\n",
            "\n",
            " [[0.70595473 0.95459    0.68013086 0.40527426 0.17757855]\n",
            "  [0.34669814 0.84836342 0.2407416  0.97626421 0.50549086]\n",
            "  [0.26732789 0.04629286 0.98035932 0.45633409 0.68915589]\n",
            "  [0.76969456 0.73812229 0.29555374 0.19130512 0.63601175]\n",
            "  [0.22052315 0.14494546 0.02909215 0.51929288 0.52732067]\n",
            "  [0.36411858 0.77024388 0.65575899 0.57907074 0.38384349]\n",
            "  [0.90078478 0.89921483 0.18450136 0.43475413 0.12638404]\n",
            "  [0.42670824 0.5596348  0.07290932 0.73039961 0.61142584]\n",
            "  [0.83886181 0.7754237  0.46413438 0.68586352 0.89984985]\n",
            "  [0.93167244 0.42530649 0.78103102 0.39071336 0.90498883]]\n",
            "\n",
            " [[0.96504944 0.66628599 0.31766    0.22190454 0.82521559]\n",
            "  [0.00860804 0.11832213 0.2575436  0.85348057 0.9833332 ]\n",
            "  [0.74846378 0.11039232 0.46740346 0.84290307 0.4462073 ]\n",
            "  [0.14388604 0.2769064  0.09101993 0.44363435 0.67956735]\n",
            "  [0.14804938 0.35386523 0.81363534 0.04063636 0.01758791]\n",
            "  [0.98702409 0.65456804 0.42052752 0.09282395 0.62490437]\n",
            "  [0.08214933 0.92668933 0.74206632 0.34567423 0.77403782]\n",
            "  [0.20911655 0.28754725 0.74759676 0.76997432 0.61500211]\n",
            "  [0.76508389 0.37686683 0.85567988 0.08350313 0.9929489 ]\n",
            "  [0.74936265 0.78963918 0.64609046 0.73823484 0.46763022]]\n",
            "\n",
            " [[0.52316605 0.46011373 0.0308881  0.89479303 0.95606648]\n",
            "  [0.9624381  0.12141372 0.63010943 0.65693408 0.02327319]\n",
            "  [0.03015011 0.4366712  0.27118054 0.64086453 0.6874182 ]\n",
            "  [0.71657034 0.81041465 0.22711808 0.5960993  0.25730244]\n",
            "  [0.06760523 0.74710298 0.93051016 0.5845166  0.00800153]\n",
            "  [0.34795138 0.69762735 0.2976812  0.69235774 0.51696918]\n",
            "  [0.89764277 0.93597666 0.02558956 0.76512801 0.62231412]\n",
            "  [0.5960617  0.81555787 0.21050618 0.90990222 0.86855671]\n",
            "  [0.59251019 0.43829191 0.77049056 0.10136391 0.67424353]\n",
            "  [0.01180143 0.28053561 0.10522693 0.87080147 0.78435303]]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Example data: 100 data points, 5 features\n",
        "X = np.random.random((100, 10, 5))  # 100 sequences, 10 time steps per sequence, 5 features\n",
        "y = np.random.random((100, 1))      # Target variable\n",
        "\n",
        "print(\"X var\")\n",
        "print(X[:5])\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(50, input_shape=(10, 5)),  # 50 LSTM units, 10 time steps, 5 features each step\n",
        "    Dense(1)                       # Output layer, predicting one value\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "#model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Fit the model\n",
        "#model.fit(X, y, epochs=20, batch_size=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Dense, TimeDistributed, Attention\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Assume X and y are prepared as in the previous examples\n",
        "\n",
        "# Build model with attention\n",
        "input_layer = Input(shape=(None, 5))\n",
        "lstm_layer = LSTM(50, return_sequences=True)(input_layer)\n",
        "attention_layer = Attention()([lstm_layer, lstm_layer])  # Self-attention\n",
        "output_layer = TimeDistributed(Dense(1))(attention_layer)\n",
        "\n",
        "print(\"input layer\")\n",
        "print(lstm_layer[:5])\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "#model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Czx-_COL-WDg",
        "outputId": "f1826711-51a7-4e9d-900d-ad9ca7959944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input layer\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, None, 50), dtype=tf.float32, name=None), name='tf.__operators__.getitem_2/strided_slice:0', description=\"created by layer 'tf.__operators__.getitem_2'\")\n"
          ]
        }
      ]
    }
  ]
}